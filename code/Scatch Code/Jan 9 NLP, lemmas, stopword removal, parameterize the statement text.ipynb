{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b19ccbc6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/46/lt114fc93v7877n5vg54yy4c0000gn/T/ipykernel_28305/3829704413.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../data/CleanedStatements/fully_catized_encoded.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf_m1/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf_m1/lib/python3.8/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         data = io.parse(\n\u001b[0m\u001b[1;32m    373\u001b[0m             \u001b[0msheet_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msheet_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf_m1/lib/python3.8/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, sheet_name, header, names, index_col, usecols, squeeze, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds)\u001b[0m\n\u001b[1;32m   1270\u001b[0m             \u001b[0mDataFrame\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mExcel\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m         \"\"\"\n\u001b[0;32m-> 1272\u001b[0;31m         return self._reader.parse(\n\u001b[0m\u001b[1;32m   1273\u001b[0m             \u001b[0msheet_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msheet_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m             \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf_m1/lib/python3.8/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, sheet_name, header, names, index_col, usecols, squeeze, dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds)\u001b[0m\n\u001b[1;32m    537\u001b[0m                 \u001b[0msheet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sheet_by_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masheetname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sheet_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msheet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msheet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"close\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m                 \u001b[0;31m# pyxlsb opens two TemporaryFiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf_m1/lib/python3.8/site-packages/pandas/io/excel/_openpyxl.py\u001b[0m in \u001b[0;36mget_sheet_data\u001b[0;34m(self, sheet, convert_float)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mScalar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0mlast_row_with_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mrow_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msheet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m             \u001b[0mconverted_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_float\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mconverted_row\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconverted_row\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf_m1/lib/python3.8/site-packages/openpyxl/worksheet/_read_only.py\u001b[0m in \u001b[0;36m_cells_by_row\u001b[0;34m(self, min_col, min_row, max_col, max_row, values_only)\u001b[0m\n\u001b[1;32m     77\u001b[0m                                  \u001b[0mdata_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                                  date_formats=self.parent._date_formats)\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmax_row\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_row\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf_m1/lib/python3.8/site-packages/openpyxl/worksheet/_reader.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# add a finaliser to close the source when this becomes possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0mtag_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtag_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdispatcher\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf_m1/lib/python3.8/xml/etree/ElementTree.py\u001b[0m in \u001b[0;36miterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1230\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1232\u001b[0;31m                 \u001b[0mpullparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1233\u001b[0m             \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpullparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_and_return_root\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpullparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf_m1/lib/python3.8/xml/etree/ElementTree.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1272\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1274\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1275\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mSyntaxError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_events_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf_m1/lib/python3.8/xml/etree/ElementTree.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1691\u001b[0m         \u001b[0;34m\"\"\"Feed encoded data to parser.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1692\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1693\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1694\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_error\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1695\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raiseerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/runner/miniforge3/conda-bld/python-split_1634073118472/work/Modules/pyexpat.c\u001b[0m in \u001b[0;36mEndElement\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf_m1/lib/python3.8/xml/etree/ElementTree.py\u001b[0m in \u001b[0;36mhandler\u001b[0;34m(tag, event, append, end)\u001b[0m\n\u001b[1;32m   1562\u001b[0m                 def handler(tag, event=event_name, append=append,\n\u001b[1;32m   1563\u001b[0m                             end=self._end):\n\u001b[0;32m-> 1564\u001b[0;31m                     \u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1565\u001b[0m                 \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEndElementHandler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1566\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mevent_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"start-ns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf_m1/lib/python3.8/xml/etree/ElementTree.py\u001b[0m in \u001b[0;36m_end\u001b[0;34m(self, tag)\u001b[0m\n\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1634\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fixname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1636\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('../../data/CleanedStatements/fully_catized_encoded.xlsx')\n",
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "# from contractions import CONTRACTION_MAP\n",
    "import unicodedata\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ce3b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b17994a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find below my suggestions:\n",
    "\n",
    "# 1. Consider the frequency of words, remove any words that occur less than 5 or 10 times,\n",
    "# this helps us get rid of unique words that are not used frequently, this way our model \n",
    "# will not focus on those words.\n",
    "\n",
    "# 2. W.r.t the above point, try removing the words that are too frequent like the one that \n",
    "# occurs more then 100-120 times, this gets rid of the higher frequency words. These \n",
    "# threshold values are configurable and solely depends on the use case, there is no right or \n",
    "# wrong way to do it.\n",
    "\n",
    "# 3. Try removing Stop words.\n",
    "\n",
    "# 4. Try Lemmatisation, it is recommended you experiment with the Lemmatisation after stop \n",
    "# words removal and then try the frequency removal method.\n",
    "\n",
    "# Try these methods and let me know if it worked for you, if not feel free to reach out in \n",
    "# the response section, Iâ€™ll try to respond sooner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f652c913",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(-2.5<df['Amount']) & (df['Amount']<-1)]['Amount'].plot.hist(bins=54,alpha=0.5)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# # Creating histogram\n",
    "# fig, ax = plt.subplots(figsize =(10, 7))\n",
    "# ax.hist(df[(-40<df['Amount']) & (df['Amount']<-3)]['Amount'], bins = [0, 25, 50, 75, 100])\n",
    "  \n",
    "# # Show plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0877e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(-40<df['Amount']) & (df['Amount']<-3)]['Amount'].plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7830d3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe7db35",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.subplots(figsize=(12,6))\n",
    "sns.barplot(df['Category'],df[abs(df['Amount']) < 400]['Amount'])\n",
    "plt.xticks(rotation = 90) \n",
    "plt.title('Spending per category')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a717ba29",
   "metadata": {},
   "outputs": [],
   "source": [
    "spends = df[df['Amount'] < 0]['Amount'].apply(lambda x: -1*x)\n",
    "cats = df[df['Amount'] < 0]['Category'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86e37ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(spends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93c1052",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.subplots(figsize=(12,6))\n",
    "plt.pie(spends)\n",
    "plt.xticks(rotation = 90) \n",
    "plt.title('Spending per category')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0af511",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775484ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('../data/CleanedStatements/fully_catized_encoded.xlsx')\n",
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import os\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "# from contractions import CONTRACTION_MAP\n",
    "import unicodedata\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "def data_corpus_create(content):\n",
    "    data_corpus=set()\n",
    "    for row in content:\n",
    "        for word in row.split(\" \"):\n",
    "            if word not in data_corpus:\n",
    "                data_corpus.add(word)\n",
    "    return sorted(data_corpus)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')\n",
    "\n",
    "content = df['Content'].copy()\n",
    "\n",
    "# build corpus, get count of each word\n",
    "nltk.FreqDist(content[0].split(' '))\n",
    "\n",
    "content1 = content.apply(lambda x: re.sub(r\"[^a-zA-Z0-9]+\", \" \", x) )\n",
    "content2 = content1.apply(lambda x: x.lower())\n",
    "content3 = content2.apply(lambda x: x.strip())\n",
    "# create corpus with frequency counts to see what can stay and what can depart\n",
    "content4 = pd.Series(sorted(content3))\n",
    "# Create a corpus for all the words\n",
    "content5 = content4.apply(remove_stopwords)\n",
    "\n",
    "content6 = content5.apply(lambda x: lemmatizer.lemmatize(x))\n",
    "content7 = content6.apply(lambda x: re.sub(\"\\d+\",\"\",x))\n",
    "cont = content7.apply(lambda x: re.sub(\"  \",\" \",x))\n",
    "cont = cont.apply(lambda x: x.strip())\n",
    "cont = cont.apply(lambda x: re.sub(\"  \",\" \",x))\n",
    "\n",
    "data_corpus = data_corpus_create(cont)\n",
    "\n",
    "# Create dictionary for each unique word, += 1 for each new encounter\n",
    "d = {}\n",
    "for row in cont:\n",
    "    for word in row.split(\" \"):\n",
    "        if word not in d.keys():\n",
    "            d[word] = 1\n",
    "        else:\n",
    "            d[word] +=1\n",
    "\n",
    "word_freq_counts = {k: v for k, v in sorted(d.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "ref = list(d.values())\n",
    "\n",
    "def remove_dict_dat(sentence):\n",
    "    s = sentence.split(\" \")\n",
    "    for word in s:\n",
    "        if d[word] < 5:\n",
    "            s.remove(word)\n",
    "        elif d[word] > 19:\n",
    "            s.remove(word)\n",
    "    return \" \".join(s)\n",
    "\n",
    "cont2 = cont.apply(remove_dict_dat)\n",
    "\n",
    "data_corpus=data_corpus_create(cont2)\n",
    "\n",
    "cont2 = cont2.apply(lambda x: x.strip())\n",
    "cont2 = cont2.apply(lambda x: lemmatizer.lemmatize(x))\n",
    "\n",
    "# converting text to integers\n",
    "token_docs = [list(set(doc.split(\" \"))) for doc in cont2]\n",
    "all_tokens = set([word for sentence in token_docs for word in sentence])\n",
    "word_to_idx = {token:idx+1 for idx, token in enumerate(all_tokens)}\n",
    "\n",
    "# converting the docs to their token ids\n",
    "X = np.array([[word_to_idx[token] for token in token_doc] for token_doc in token_docs], dtype=object)\n",
    "\n",
    "# padding the sequences\n",
    "X_padded = pad_sequences(X, padding=\"post\")\n",
    "\n",
    "# converting to pandas df\n",
    "X_df = pd.DataFrame(X_padded)\n",
    "\n",
    "X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368bde70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "sentences = pd.Series(token_docs)\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb_df = df.join(pd.DataFrame(mlb.fit_transform(sentences),columns=mlb.classes_))\n",
    "mlb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475eb96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee16a5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change numeric columns to str for the transaction text\n",
    "lis = []\n",
    "\n",
    "for x in X_df.columns:\n",
    "    lis.append('word'+str(x))\n",
    "X_df.columns = lis\n",
    "\n",
    "# combine original dataframe to the parameterized content dataframe\n",
    "dff = pd.concat([df,X_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a843af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir('..')\n",
    "from auto_budget import bank_functions_NWFCU as nw\n",
    "ref = nw.ref_dictionaries(dff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14caf5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just ordinal encoding, about 63% accuracy no cross-validation\n",
    "\n",
    "# change numeric columns to str for the transaction text\n",
    "lis = []\n",
    "\n",
    "for x in X_df.columns:\n",
    "    lis.append('word'+str(x))\n",
    "X_df.columns = lis\n",
    "\n",
    "# combine original dataframe to the parameterized content dataframe\n",
    "dff = pd.concat([df,X_df],axis=1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# separate predictors and response\n",
    "X = dff.drop(['Purchase Date','Purchase Time','Payment_Method','Verification Date', 'Content', 'Category','PT_total_seconds','Payment_Method_Cat', 'Category_Cat', 'Content'], axis=1)\n",
    "y = dff['Category_Cat'].copy()\n",
    "\n",
    "# test train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "dectree = DecisionTreeClassifier()\n",
    "dectree.fit(X_train, y_train)\n",
    "predictions = dectree.predict(X_test)\n",
    "sum(predictions == y_test.array) / len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb3bba8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# one hot encoding but with redundancy\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown ='ignore')\n",
    "enc_df = pd.DataFrame(enc.fit_transform(X_df).toarray())\n",
    "\n",
    "# change numeric columns to str for the transaction text\n",
    "lis = []\n",
    "\n",
    "for x in enc_df.columns:\n",
    "    lis.append('word'+str(x))\n",
    "enc_df.columns = lis\n",
    "\n",
    "# combine original dataframe to the parameterized content dataframe\n",
    "dff = pd.concat([df,enc_df],axis=1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# separate predictors and response\n",
    "X = dff.drop(['Purchase Date','Purchase Time','Payment_Method','Verification Date', 'Content', 'Category','PT_total_seconds','Payment_Method_Cat', 'Category_Cat', 'Content'], axis=1)\n",
    "y = dff['Category_Cat'].copy()\n",
    "\n",
    "# test train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "dectree = DecisionTreeClassifier()\n",
    "dectree.fit(X_train, y_train)\n",
    "predictions = dectree.predict(X_test)\n",
    "sum(predictions == y_test.array) / len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3629dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding without redundancy\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "sentences = pd.Series(token_docs)\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb_df = df.join(pd.DataFrame(mlb.fit_transform(sentences),columns=mlb.classes_))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# separate predictors and response\n",
    "X = mlb_df.drop(['Purchase Date','Purchase Time','Payment_Method','Verification Date', 'Content', 'Category','PT_total_seconds','Payment_Method_Cat', 'Category_Cat', 'Content'], axis=1)\n",
    "y = mlb_df['Category_Cat'].copy()\n",
    "\n",
    "# test train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "dectree = DecisionTreeClassifier()\n",
    "dectree.fit(X_train, y_train)\n",
    "predictions = dectree.predict(X_test)\n",
    "sum(predictions == y_test.array) / len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daeacc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no transaction words at all\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# separate predictors and response\n",
    "X = df.drop(['Purchase Date','Purchase Time','Payment_Method','Verification Date', 'Content', 'Category','PT_total_seconds','Payment_Method_Cat', 'Category_Cat', 'Content'], axis=1)\n",
    "y = df['Category_Cat'].copy()\n",
    "\n",
    "# test train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "dectree = DecisionTreeClassifier()\n",
    "dectree.fit(X_train, y_train)\n",
    "predictions = dectree.predict(X_test)\n",
    "sum(predictions == y_test.array) / len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f371cba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure out how to use index-based words in sklearn \n",
    "# https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#tokenizing-text-with-scikit-learn\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plot\n",
    "import warnings as w\n",
    "%matplotlib inline\n",
    "\n",
    "# supress warnings\n",
    "w.filterwarnings('ignore')\n",
    "\n",
    "num_simulations = 10000\n",
    "# results = pd.DataFrame(columns=['trial','no_words','words_ordinal','words_onehot'])\n",
    "results = pd.DataFrame()\n",
    "dectree = DecisionTreeClassifier()\n",
    "\n",
    "def feature_selection(X,y):\n",
    "    # initial fit\n",
    "    dectree.fit(X, y)\n",
    "    model = SelectFromModel(dectree, prefit=True)\n",
    "    X_new = model.transform(X)\n",
    "    return X_new\n",
    "\n",
    "def decision_tree_results(prepped_df, featselect=False, dectree):\n",
    "    \"\"\"runs split decision tree model and returns percent of matches\"\"\"\n",
    "    # separate predictors and response\n",
    "    X = prepped_df.drop(['Purchase Date','Purchase Time','Payment_Method','Verification Date', 'Content', 'Category','PT_total_seconds','Payment_Method_Cat', 'Category_Cat', 'Content'], axis=1)\n",
    "    y = prepped_df['Category_Cat'].copy()\n",
    "    \n",
    "    if featselect:\n",
    "        X_new = feature_selection(X, y)\n",
    "    else:\n",
    "        X_new = X.copy()\n",
    "\n",
    "    # test train split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "    # dectree = DecisionTreeClassifier()\n",
    "    dectree.fit(X_train, y_train)\n",
    "    predictions = dectree.predict(X_test)\n",
    "    return (sum(predictions == y_test.array) / len(predictions)) * 100\n",
    "\n",
    "# run 1,000 simulations for each dataset, and see which has the best misclassification rate\n",
    "\n",
    "for i in range(num_simulations):\n",
    "    # no_words = decision_tree_results(df)\n",
    "    # words_ordinal = decision_tree_results(dff)\n",
    "    words_onehot = decision_tree_results(mlb_df, dectree)\n",
    "    words_onehot_featselect = decision_tree_results(mlb_df,True,dectree)\n",
    "    #results = results.append({'trial':i,'no_words':no_words,'words_ordinal':words_ordinal,'words_onehot':words_onehot}, \\\n",
    "    #                         ignore_index=True)\n",
    "    results = results.append({'trial':i,'words_onehot':words_onehot,'words_onehot_featselect':words_onehot_featselect}, \\\n",
    "                            ignore_index=True)\n",
    "\n",
    "# pivot dataframe to use seaborn hue\n",
    "\n",
    "results_melted = pd.melt(results,id_vars=['trial'])\n",
    "\n",
    "# plot results in graph\n",
    "plt.figure(figsize=(16,10))\n",
    "sns.lineplot(x='trial',y='value',hue='variable',data=results_melted)\n",
    "plt.title('Comparison of datasets in decision tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d293dd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb_df[mlb_df.columns[26:]].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc018d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection for the mlb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8a0687",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e2d390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot with feature selection\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# separate predictors and response\n",
    "X = mlb_df.drop(['Purchase Date','Purchase Time','Payment_Method','Verification Date', 'Content', 'Category','PT_total_seconds','Payment_Method_Cat', 'Category_Cat', 'Content'], axis=1)\n",
    "y = mlb_df['Category_Cat'].copy()\n",
    "\n",
    "# decision tree model\n",
    "dectree = DecisionTreeClassifier()\n",
    "\n",
    "# initial fit\n",
    "dectree.fit(X, y)\n",
    "model = SelectFromModel(dectree, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "\n",
    "# test train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# decision tree model\n",
    "dectree.fit(X_train, y_train)\n",
    "predictions = dectree.predict(X_test)\n",
    "(sum(predictions == y_test.array) / len(predictions)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d678bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['featselect'] = words_onehot_featselect >= results.words_onehot\n",
    "sum(results.featselect) / len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251220a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc347bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "os.chdir('code')\n",
    "mlb_df = pd.read_excel('mlb_df.xlsx')\n",
    "mlb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1344cd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bb0ab4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# figure out how to use index-based words in sklearn \n",
    "# https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#tokenizing-text-with-scikit-learn\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings as w\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# supress warnings\n",
    "w.filterwarnings('ignore')\n",
    "\n",
    "num_simulations = 10000\n",
    "# results = pd.DataFrame(columns=['trial','no_words','words_ordinal','words_onehot'])\n",
    "results = pd.DataFrame()\n",
    "dectree = DecisionTreeClassifier()\n",
    "ranfore = RandomForestClassifier(n_estimators=17)\n",
    "\n",
    "def feature_selection(X,y):\n",
    "    # initial fit\n",
    "    dectree.fit(X, y)\n",
    "    model = SelectFromModel(dectree, prefit=True)\n",
    "    X_new = model.transform(X)\n",
    "    return X_new\n",
    "\n",
    "def model_evaluation(prepped_df, model):\n",
    "    \"\"\"runs split decision tree model and returns percent of matches\"\"\"\n",
    "    # separate predictors and response\n",
    "    X = prepped_df.drop(['Purchase Date','Purchase Time','Payment_Method','Verification Date', 'Content', 'Category','PT_total_seconds','Payment_Method_Cat', 'Category_Cat', 'Content'], axis=1)\n",
    "    y = prepped_df['Category_Cat'].copy()\n",
    "    \n",
    "    # Feature selection\n",
    "    X_new = feature_selection(X, y)\n",
    "\n",
    "    # test train split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    return (sum(predictions == y_test.array) / len(predictions)) * 100\n",
    "\n",
    "# run 1,000 simulations for each dataset, and see which has the best misclassification rate\n",
    "\n",
    "for i in range(num_simulations):\n",
    "    # no_words = decision_tree_results(df)\n",
    "    # words_ordinal = decision_tree_results(dff)\n",
    "    decisiontree = model_evaluation(mlb_df, dectree)\n",
    "    randomforest = model_evaluation(mlb_df,ranfore)\n",
    "    #results = results.append({'trial':i,'no_words':no_words,'words_ordinal':words_ordinal,'words_onehot':words_onehot}, \\\n",
    "    #                         ignore_index=True)\n",
    "    results = results.append({'trial':i,'Decision Tree':decisiontree,'Random Forest':randomforest}, \\\n",
    "                            ignore_index=True)\n",
    "\n",
    "# pivot dataframe to use seaborn hue\n",
    "\n",
    "results_melted = pd.melt(results,id_vars=['trial'])\n",
    "\n",
    "# plot results in graph\n",
    "plt.figure(figsize=(16,10))\n",
    "sns.lineplot(x='trial',y='value',hue='variable',data=results_melted)\n",
    "plt.title('Comparison of models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b75b582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload historic data and train decision tree model \n",
    "# upload new data, clean, encode and categorize using model\n",
    "# review decision tree's categorization against my own\n",
    "# add new encoded data to historic data\n",
    "# aggregate new data by category and upload as new sheet to budget"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
